{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6255337a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfcd40a4a6d9da75b47efd231cb2e043",
     "grade": true,
     "grade_id": "cell-2020906bd78d79d5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "\n",
    "Group nr:\n",
    "\n",
    "Name 1 and CID: Fredrik Sand√©n (Sandenfr)\n",
    "\n",
    "Name 2 and CID: Simon Holmgren (Simh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb017ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa26dfc2f35f895dabff1310042d379f",
     "grade": false,
     "grade_id": "cell-14bb5073a539cbd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mining_world import Environment\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64aea2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6903b1a761cb59cbd2251b2033afc820",
     "grade": false,
     "grade_id": "cell-7aa6c85811a632c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This assignment dels with data structures, K-NN, tree-based classifiers and evaluation of the methods deployed on a fictional scenario.  \n",
    "\n",
    "## Mining world \n",
    "\n",
    "<img src=\"imgs/poster.png\" width=\"800\"/>\n",
    "\n",
    "## Scenario\n",
    "\n",
    "\n",
    "Humanity has now reached a point where we need to extract and refine more Copium, a precious resource with great value. The only problem is that Copium can only be found on certain uninhabitable planets. This of course means that automated robots are sent instead.      \n",
    "\n",
    "Copium is naturally very unstable and is only exists very temporary before it decays. There are very specific geological activities and circumstances needed for copium to form. The life cycle of Copium follows. First, a hot stream of liquid magma flows to the surface, creating a hotspot that that looks like a small crater. At the surface, if the conditions are correct, copium can form during the cool-down period. But as stated previously, Copium is unstable in its natural environment and decays to other materials shortly after. \n",
    "\n",
    "The formation of these deposits craters are very random, but the heat from them can easilly be detected with a satellite. But there is no way of knowing if the newly formed depoist contains copium from just a satellite, therefor there is a robot rover on the ground with sensors that can collect further measurements. The rover has many sensors that can measure the properties of the ground below it, but of course, Copium can not directly be detected with these types of sensors. This is where the machine learning approach will be used, to take all those measurements and try to classify if the deposit contains Copium or not.  \n",
    "\n",
    "\n",
    "<img src=\"imgs/overView.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad74f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14ffb7e00ae02f2be65dafa3854c8b57",
     "grade": false,
     "grade_id": "cell-80e56d230a18f7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Navigation - Tree search\n",
    "\n",
    "This section will show how the naivigation is done. This is not a part of the assignment to understand, but will be used. \n",
    "\n",
    "##  Breadth first\n",
    "\n",
    "The method used is a breadth first search algorithm, it is one of the simplest tree search algorithms and basically tries every option for a fixed number of steps and chooses the best one. The robot only have 4 actions, it can move at every time step in a chosen direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606296fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "128f22d73852ea6a5372822b0b37c784",
     "grade": false,
     "grade_id": "cell-c992a6248dd856e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Actions \n",
    "\n",
    "<img src=\"imgs/actions.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5627207",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32d128b3906d7686ac730d0aca4908af",
     "grade": false,
     "grade_id": "cell-9a8f005a81419a11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, actor):\n",
    "        self.actor = actor\n",
    "        self.total_score = 0\n",
    "    \n",
    "    def update(self, action, inherited_score):\n",
    "        score = self.actor.step(action)\n",
    "        self.total_score = 1.05*inherited_score + score\n",
    "        return self.total_score\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afda62f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb47522e94f5a7e21138374b9cca9f85",
     "grade": false,
     "grade_id": "cell-7e1a22235cf88444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def breadth_first_search(actor, max_depth, action_space):\n",
    "    node = Node(copy.deepcopy(actor)) \n",
    "    queue_keys = ['0'] # queue to keep track of nodes that has not yet been expanded.  \n",
    "    visited = {queue_keys[0]: node} # saves visited nodes in order to not recalulate the entire path for each step. \n",
    "    \n",
    "    max_score = -np.inf\n",
    "    best_action = None\n",
    "\n",
    "    while True:\n",
    "        key = queue_keys.pop(0)\n",
    "        if len(key) > max_depth: # stop at a set depth \n",
    "            break    \n",
    "        node = visited[key]\n",
    "        \n",
    "        for action in action_space: # expand all children nodes\n",
    "            child_node = copy.deepcopy(node)  # copy current node\n",
    "            score = child_node.update(action=action, inherited_score=node.get_score()) # update node with action\n",
    "            child_key = key + action # create child node key\n",
    "            \n",
    "            if score > max_score: # save best path \n",
    "                max_score = score\n",
    "                best_action = child_key[1]\n",
    "                \n",
    "            visited[child_key] = child_node  # add child node to visited nodes.\n",
    "            queue_keys.append(child_key)  # add child node queue of non expanded nodes. \n",
    "            \n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68fa06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f80922e17efb57f64f6dbbfd7c7225d9",
     "grade": false,
     "grade_id": "cell-bc5397658a16ae14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Collect data\n",
    "\n",
    "The first step is to collect data that will be used for both training and validation. The available types features can be seen with env.get_sensor_properties() and the actual measurements can be retrieved with env.get_sensor_readings(). It will return a dictionary with the same keys as in env.get_sensor_properties() containg a value for each feature. If the robot is not currently over a deposit, then it will return None. The true label can be extracted with env.get_ground_truth(), which will return a 1 if there is copium in the deposit and 0 if not. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5586ce9b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3a154aa52faef7af4429dcfeca47967",
     "grade": true,
     "grade_id": "cell-564b8441165efae4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Have True for first time running and then set to False. \n",
    "collect_data = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a864720",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "731bc4a9f7eefb5692544b4896a2e748",
     "grade": false,
     "grade_id": "cell-e5ae97d8d9debdab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor properties ['ground_density', 'moist', 'reflectivity', 'silicon_rate', 'oxygen_rate', 'iron_rate', 'aluminium_rate', 'magnesium_rate', 'undetectable']\n"
     ]
    }
   ],
   "source": [
    "env = Environment(map_type=1, fps=500, resolution=(1000, 1000))\n",
    "env.exit()\n",
    "sensor_properties = env.get_sensor_properties()\n",
    "print('Sensor properties', sensor_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77dc7249",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e494e8fce0535af86d13d035a8e10603",
     "grade": false,
     "grade_id": "cell-060d3a2c29bfe32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if collect_data == True:\n",
    "    env = Environment(map_type=1, fps=500, resolution=(1000, 1000))\n",
    "    sensor_properties = env.get_sensor_properties()\n",
    "\n",
    "    # We can initilize the dictionary the following way.\n",
    "    data = dict()\n",
    "    data['copium'] = [] \n",
    "    for key in sensor_properties:\n",
    "        data[key] = []\n",
    "\n",
    "    for i in range(5000):\n",
    "        action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "        env.step(action)\n",
    "        # if we are over a deposit. \n",
    "        if env.get_sensor_readings() is not None:\n",
    "            sensor_readings = env.get_sensor_readings()\n",
    "            copium = env.get_ground_truth()\n",
    "\n",
    "            for key in sensor_readings:\n",
    "                data[key].append(sensor_readings[key])\n",
    "            data['copium'].append(copium)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    env.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96abd44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf20cd270c452764c144604d39f9d328",
     "grade": false,
     "grade_id": "cell-5d9009d13bc1d8f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exerscie 1: Data structure\n",
    "\n",
    "## a) Pandas data frame \n",
    "In this assignment we will work pandas data frame for storing the collected data. First create a pandas data frame from the dictionary. The documentation for it can be found at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html, only the data feild needs to be filled in with the created dictionary. Call this data frame df. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ae59e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18afbbd66a35181cac0d3f8d649981b8",
     "grade": false,
     "grade_id": "cell-733f079ee3c0ac51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: create pandas dataframe\n",
    "if collect_data == True:\n",
    "    df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341255d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9a1e3412453720dd6456a9477872245",
     "grade": false,
     "grade_id": "cell-79c4c3bbd4567c1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "## b) Save data\n",
    "Collecting data for each time the notebook is opened is time consuming and the tests are not as reproducable if the data is different each time. Therefore we save the dateframe and we can load the data instead. After this you can set collect_data=False above. Use the function pandas function df.to_csv('name_of_file.csv', index=False)\n",
    "\n",
    "The task is to save the data in a file named mydata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22f5dd0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c9b2f586a4f5a8444748e5322d74957",
     "grade": true,
     "grade_id": "cell-ff6790fec729ba0a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: save data to a csv file named mydata.  \n",
    "if collect_data == True:\n",
    "    df.to_csv('mydata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ceccb6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92cbcd77e462c7f4c1ff91f33a467871",
     "grade": false,
     "grade_id": "cell-387be287dd96e402",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) Load data\n",
    "The task is to load the data from mydata.csv into a dataframe named df. Use the function df = pd.read_csv('name_of_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0c91dd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e370fd9485b66347115ffc5e77c0a29d",
     "grade": true,
     "grade_id": "cell-9db787fbf7a3f989",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: read data from mydata.csv into a dataframe called df.  \n",
    "df = pd.read_csv('mydata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff64c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64fbf2ce60f9ad95619b97b43062926e",
     "grade": false,
     "grade_id": "cell-ebfce4b848cdc9f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data for a feature \n",
      " 0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1590    0\n",
      "1591    1\n",
      "1592    0\n",
      "1593    0\n",
      "1594    0\n",
      "Name: copium, Length: 1595, dtype: int64\n",
      "\n",
      "Single sample from index \n",
      " copium            0.000000\n",
      "ground_density    1.843045\n",
      "moist             0.241389\n",
      "reflectivity      0.569418\n",
      "silicon_rate      0.120205\n",
      "oxygen_rate       0.018468\n",
      "iron_rate         0.653094\n",
      "aluminium_rate    0.096327\n",
      "magnesium_rate    0.065495\n",
      "undetectable      0.046411\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "All features without copium \n",
      "       ground_density     moist  reflectivity  silicon_rate  oxygen_rate  \\\n",
      "0           0.739599  0.107341      0.490689      0.020877     0.043239   \n",
      "1           1.843045  0.241389      0.569418      0.120205     0.018468   \n",
      "2           0.400000  0.131172      0.301149      0.047718     0.068963   \n",
      "3           1.439975  0.188397      0.125550      0.139022     0.032345   \n",
      "4           0.400000  0.224848      0.493306      0.181588     0.000502   \n",
      "...              ...       ...           ...           ...          ...   \n",
      "1590        2.094507  0.211981      0.563414      0.225379     0.019179   \n",
      "1591        1.128566  0.039508      0.172457      0.182309     0.042205   \n",
      "1592        2.349983  0.097475      0.398449      0.100457     0.029907   \n",
      "1593        2.577069  0.289586      0.363439      0.154962     0.065273   \n",
      "1594        1.050521  0.256307      0.166161      0.158596     0.036581   \n",
      "\n",
      "      iron_rate  aluminium_rate  magnesium_rate  undetectable  \n",
      "0      0.633467        0.232851        0.039956      0.029610  \n",
      "1      0.653094        0.096327        0.065495      0.046411  \n",
      "2      0.588818        0.169044        0.076056      0.049400  \n",
      "3      0.487409        0.238032        0.067029      0.036162  \n",
      "4      0.415309        0.232589        0.130955      0.039057  \n",
      "...         ...             ...             ...           ...  \n",
      "1590   0.364806        0.251599        0.095229      0.043807  \n",
      "1591   0.638557        0.037174        0.050409      0.049346  \n",
      "1592   0.502569        0.183684        0.130589      0.052793  \n",
      "1593   0.435803        0.179573        0.061552      0.102837  \n",
      "1594   0.377490        0.274279        0.127593      0.025463  \n",
      "\n",
      "[1595 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copium</th>\n",
       "      <th>ground_density</th>\n",
       "      <th>moist</th>\n",
       "      <th>reflectivity</th>\n",
       "      <th>silicon_rate</th>\n",
       "      <th>oxygen_rate</th>\n",
       "      <th>iron_rate</th>\n",
       "      <th>aluminium_rate</th>\n",
       "      <th>magnesium_rate</th>\n",
       "      <th>undetectable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.170533</td>\n",
       "      <td>1.413588</td>\n",
       "      <td>0.150130</td>\n",
       "      <td>0.300139</td>\n",
       "      <td>0.137604</td>\n",
       "      <td>0.041467</td>\n",
       "      <td>0.403441</td>\n",
       "      <td>0.259109</td>\n",
       "      <td>0.107806</td>\n",
       "      <td>0.050573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.376218</td>\n",
       "      <td>0.642583</td>\n",
       "      <td>0.086313</td>\n",
       "      <td>0.176773</td>\n",
       "      <td>0.076981</td>\n",
       "      <td>0.027730</td>\n",
       "      <td>0.160909</td>\n",
       "      <td>0.127567</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.029651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.054140</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>0.006451</td>\n",
       "      <td>0.005778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.933412</td>\n",
       "      <td>0.078233</td>\n",
       "      <td>0.145155</td>\n",
       "      <td>0.076663</td>\n",
       "      <td>0.019467</td>\n",
       "      <td>0.274809</td>\n",
       "      <td>0.161031</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>0.027935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.297188</td>\n",
       "      <td>0.148299</td>\n",
       "      <td>0.301798</td>\n",
       "      <td>0.131738</td>\n",
       "      <td>0.039022</td>\n",
       "      <td>0.423402</td>\n",
       "      <td>0.255889</td>\n",
       "      <td>0.100420</td>\n",
       "      <td>0.046725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.803201</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.456903</td>\n",
       "      <td>0.182463</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.526973</td>\n",
       "      <td>0.343514</td>\n",
       "      <td>0.146767</td>\n",
       "      <td>0.066417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.497465</td>\n",
       "      <td>0.299798</td>\n",
       "      <td>0.599848</td>\n",
       "      <td>0.470706</td>\n",
       "      <td>0.193886</td>\n",
       "      <td>0.793212</td>\n",
       "      <td>0.687683</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>0.209317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            copium  ground_density        moist  reflectivity  silicon_rate  \\\n",
       "count  1595.000000     1595.000000  1595.000000   1595.000000   1595.000000   \n",
       "mean      0.170533        1.413588     0.150130      0.300139      0.137604   \n",
       "std       0.376218        0.642583     0.086313      0.176773      0.076981   \n",
       "min       0.000000        0.400000     0.000147      0.001082      0.011549   \n",
       "25%       0.000000        0.933412     0.078233      0.145155      0.076663   \n",
       "50%       0.000000        1.297188     0.148299      0.301798      0.131738   \n",
       "75%       0.000000        1.803201     0.226337      0.456903      0.182463   \n",
       "max       1.000000        4.497465     0.299798      0.599848      0.470706   \n",
       "\n",
       "       oxygen_rate    iron_rate  aluminium_rate  magnesium_rate  undetectable  \n",
       "count  1595.000000  1595.000000     1595.000000     1595.000000   1595.000000  \n",
       "mean      0.041467     0.403441        0.259109        0.107806      0.050573  \n",
       "std       0.027730     0.160909        0.127567        0.064500      0.029651  \n",
       "min       0.000212     0.054140        0.020606        0.006451      0.005778  \n",
       "25%       0.019467     0.274809        0.161031        0.056553      0.027935  \n",
       "50%       0.039022     0.423402        0.255889        0.100420      0.046725  \n",
       "75%       0.058084     0.526973        0.343514        0.146767      0.066417  \n",
       "max       0.193886     0.793212        0.687683        0.369549      0.209317  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the data frame you can access all data for a key with for example:\n",
    "print(\"All data for a feature \\n\", df[\"copium\"])\n",
    "print()\n",
    "\n",
    "# You can access a single sample with:\n",
    "print(\"Single sample from index \\n\", df.iloc[1])\n",
    "print()\n",
    "\n",
    "# You can access all freatures but one with:\n",
    "all_features_without_copium = df.drop(columns='copium')\n",
    "print(\"All features without copium \\n\", all_features_without_copium)\n",
    "\n",
    "# To get an overview \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7a9bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70f5b5f338fbfd7148f2f4d1073bbd2b",
     "grade": false,
     "grade_id": "cell-90bd2839fd455f3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) Part 1: Split data\n",
    "\n",
    "Here we will devide the data into a training set and a test set. Good rule of thumb is to use 80% of the data in the training set and 20 % in the test set. The the two data sets should be randomly sampled (shuffle=True), we also set he seed with random_state so that the split is reproducable. This is done with train_test_split() from sklearn, https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. The syntax looks like \n",
    "\n",
    "train, test = train_test_split(dataframe, test_size=ratio_test_set, shuffle=True, random_state=42)\n",
    "\n",
    "Why is it important that the data is shuffled when it is split, what could happen otherwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45397d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53bce3ebb6b20c538748a546b7fd13c5",
     "grade": true,
     "grade_id": "cell-e9dac1f1c72ed455",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\\\n",
    "Since we want to train the modle for the entire data set. If we don't shuffle it we will only learn the modle on ex the first 80% of the data, missing the last 20%. So it's better to use the shuffled data instead. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e21475",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65406c33cc5ec7553e81ad195c39129b",
     "grade": true,
     "grade_id": "cell-d3da8450794a3f7d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: devide the data into train and test set. \n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35d89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32af0aaf1e8fa024b7091f469b51ac11",
     "grade": false,
     "grade_id": "cell-df82c101fb36b17b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Part 1: Analyse data balance\n",
    "\n",
    "The occurance can be retrevied with .value_counts() from a pandas date frame. Here get the occurance of copium in the samples. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c7e1f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91bac4c7e7622383262c58b7e7a257df",
     "grade": true,
     "grade_id": "cell-7c51441927cdeb22",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with copium: 272\n",
      "Sample without copium: 1323\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of samples with copium and the number of samples without copium.\n",
    "dfc = df.value_counts(\"copium\")\n",
    "print(\"Sample with copium:\", dfc[1])\n",
    "print(\"Sample without copium:\", dfc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeccc46",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec667850e64bd69400083d01a46ed070",
     "grade": true,
     "grade_id": "cell-49be72b9d48244e2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\\\n",
    "Since the samples with copium is not equal to the the amount of samples with copium the dataset is inbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801c409",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76f3b50c8e1733fc0206b99f3581c09",
     "grade": false,
     "grade_id": "cell-116683159a32e20b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## e) Part 2: Balance data\n",
    "\n",
    "We will throughout this assignment test each method with the original dataset and a balanced dataset. We show how balanceing can be done by downsampling below. Your task is to create an upsampled balanced dataset. You only need to use the upsampled data set for the rest of the other part 2) exerices. \n",
    "\n",
    "What could be the reason for chosing either upsampled or downsampeled metohd to balance the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f727f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5f76e04d0efc78cb1d663dd6823e930",
     "grade": true,
     "grade_id": "cell-605878f4821edafc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\\\n",
    "To get an optimal amount of datapoints. There may be issues if the amount of datapoints is to low or to high. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3437d2f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e969f22787b777174428e22dc930e9cd",
     "grade": false,
     "grade_id": "cell-63de30dce9039bf1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampled data: copium\n",
      "0    1055\n",
      "1    1055\n",
      "Name: count, dtype: int64\n",
      "Downsampled data: copium\n",
      "0    221\n",
      "1    221\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO balance data det. \n",
    "# Seperate the data into something that contains copium and one that doesn't,\n",
    "# can for example be done with train[train[\"copium\"]==0] etc.\n",
    "train_zero = train[train[\"copium\"]==0]\n",
    "train_one = train[train[\"copium\"]==1]\n",
    "\n",
    "# downsample majority\n",
    "train_zero_downsampled = resample(train_zero,\n",
    "                               n_samples=train_one.shape[0])\n",
    "\n",
    "train_balanced_downsampled = pd.concat([train_one, train_zero_downsampled])\n",
    "\n",
    "\n",
    "# TODO: upsample minority \n",
    "train_one_upsampled = resample(train_one,\n",
    "                               n_samples=train_zero.shape[0])\n",
    "\n",
    "train_balanced_upsampled = pd.concat([train_zero, train_one_upsampled])\n",
    "\n",
    "\n",
    "train_balanced_downsampled_count = train_balanced_downsampled.value_counts(\"copium\")\n",
    "train_balanced_upsampled_count = train_balanced_upsampled.value_counts(\"copium\")\n",
    "print(\"Upsampled data:\",train_balanced_upsampled_count)\n",
    "print(\"Downsampled data:\", train_balanced_downsampled_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1d537",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61b1cd3925a3265c2c4bbcada0fb0d1d",
     "grade": false,
     "grade_id": "cell-073e09c001bc6760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: Performance evaluation\n",
    "\n",
    "Here we will define a class that later will be used for evaluation the performance of the classification methods. It will be used to log the accuracy, precsision and recalll. More information about precision and recall can be found at https://en.wikipedia.org/wiki/Precision_and_recall. \n",
    "\n",
    "Expliain why the different metics are usefull and why is not always accuarcy enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a91be1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ffd749262a2e421ca7d3f10a7360587",
     "grade": true,
     "grade_id": "cell-c17f13f39f1c121d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3904d3a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8296987d3c96bc619d6b129006384a",
     "grade": false,
     "grade_id": "cell-30d4bb59031d814a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Classification_eval(object):\n",
    "    def __init__(self):\n",
    "        # counters \n",
    "        self.TP = 0 # Predicted positive, positive label\n",
    "        self.FP = 0 # Predicted positive, negative label\n",
    "        self.TN = 0 # Predicted negative, negative label\n",
    "        self.FN = 0 # Predicted negative, positive label. \n",
    "    \n",
    "    def update(self, pred, label):\n",
    "        \"\"\"\n",
    "        pred - is the prediction will be either a 1 or 0. \n",
    "        label - is the correct answer, will be either a 1 or 0.\n",
    "        \"\"\"\n",
    "        # TODO: add to one of the correct counter each time this function is called. \n",
    "        if pred and label: self.TP += 1\n",
    "        if pred and not label: self.FP += 1\n",
    "        if not pred and not label: self.TN += 1\n",
    "        if not pred and label: self.FN += 1\n",
    "\n",
    "    def accuracy(self): \n",
    "        # returns the accuracy \n",
    "        if (self.TP + self.TN) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the accuracy.\n",
    "        accuracy = (self.TP + self.TN)/(self.TP + self.TN + self.FP + self.FN)\n",
    "        return np.round(accuracy, 4)\n",
    "    \n",
    "    def precision(self): # percentage of the estimated positive that actually is positive\n",
    "        if (self.TP + self.FP) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the precision.\n",
    "        \n",
    "        precision = self.TP/(self.TP + self.FP)\n",
    "        return np.round(precision, 4)\n",
    "    \n",
    "    def recall(self): # percentage of correctly identified positive of the total positive\n",
    "        if (self.TP + self.FN) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the recall.\n",
    "        recall = (self.TP)/(self.TP + self.FN)\n",
    "        return np.round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef15a4bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28b02c576017e0b9014fa4ff9122c23c",
     "grade": true,
     "grade_id": "cell-9f928939ef661cd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n"
     ]
    }
   ],
   "source": [
    "# Test Classification_eval\n",
    "\n",
    "test_eval = Classification_eval()\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(1,1)\n",
    "test_eval.update(0,1)\n",
    "test_eval.update(1,0)\n",
    "test_eval.update(1,0)\n",
    "test_eval.update(0,0)\n",
    "test_eval.update(0,0)\n",
    "test_eval_ = np.allclose([test_eval.accuracy(), test_eval.precision(), test_eval.recall()], [0.625, 0.6, 0.75])\n",
    "if test_eval_:\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "assert test_eval_\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02359150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc9f9d30243f35da0eb921d1ed9df9d6",
     "grade": false,
     "grade_id": "cell-42e7fff982a76017",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 4: K- nearest neighbours\n",
    "\n",
    "## a) Normalize\n",
    "\n",
    "Here we will code our K-NN classifier, method 2.1 on page 21 in the book has the psudo code for K-NN. We will start with the data normalization, i.e. we will normalize the input data so that each feature has the same range in terms of max/min values. The min value can be found with data.min(), similarly for the max value. \n",
    "\n",
    "Why is it important that the data is normalized for the K-NN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c067ca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30acb00aaff6fd9caefa4e560880300a",
     "grade": true,
     "grade_id": "cell-96776ce3a5e4ac7d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Text answer:\\\n",
    "To make sure you are searching in the values from smallest to biggest and avoid clumping them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31958a4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a3b2b21a5d0cb1463a24b5d74c1b440",
     "grade": false,
     "grade_id": "cell-1965136e57f66697",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        # normalize the data and return it. \n",
    "        return (data-self.min)/(self.max-self.min)\n",
    "    \n",
    "    def update_normalization(self, data):\n",
    "        # Save the min and max values for each feature. This funciton is only used for the training data.\n",
    "        self.min = data.min()\n",
    "        self.max = data.max()\n",
    "        return self.normalize(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7b558",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5f9e33e5a86d44f328fe1e0f9c2cdf7",
     "grade": false,
     "grade_id": "cell-eb1b5ea8fa8ff1ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b) K-NN\n",
    "Lets make the k-Nearest Neighbors (k-NN) algorithm, fill in the TODO. \n",
    "\n",
    "The core idea behind k-NN is to make predictions based on the \"neighborhood\" of data points in a feature space. In this impementation we will use the $l^2$-norm for distance to determine the k nearest samples. The classification is done with a majoiry vote of the k nearest samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94641da4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8da0607f38f74d3cc66f3a0be3192952",
     "grade": false,
     "grade_id": "cell-85c6a185febcb58a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class KNN(object):\n",
    "    def __init__(self, k):\n",
    "        self.features = None # normalized features from training data \n",
    "        self.labels = None # the corresponding labels (if there is copium)\n",
    "        self.normalize = Normalize() # class instance for normalization\n",
    "        self.k = k # the k value in k-nn algorithm.\n",
    "        \n",
    "    def fit(self, features, labels):\n",
    "        # This is where we save the training data. \n",
    "        # TODO: update the normalize filter and normalize the features (save to self.features)\n",
    "        # and save the labels to self.labels. Hint see cell bellow for how the inputs looks like\n",
    "        \n",
    "        self.features = self.normalize.update_normalization(features)\n",
    "        self.labels = labels\n",
    "\n",
    "    def predict(self, sample):\n",
    "        # For this part you are not allowed to use an existing K-NN package. \n",
    "        # This function takes one sample as an input and outputs a predicion \n",
    "        \n",
    "        # TODO: \n",
    "        # First normalize the input sample. \n",
    "        # Find the k closest samples in self.features to the input sample\n",
    "        # Return the prediction from the majority vote.  \n",
    "        normalized_sample = self.normalize.normalize(sample)\n",
    "        \n",
    "        distance_list = [sum((self.features.iloc[i]-normalized_sample)**2) for i in range(self.features.shape[0])]\n",
    "\n",
    "        k_index = np.argsort(distance_list)[:self.k]\n",
    "\n",
    "        copium = [self.labels.iloc[i] for i in k_index]\n",
    "\n",
    "        return np.bincount(copium).argmax()\n",
    "            \n",
    "        #return predicion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce32e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4cccdb1ba0f9041ddc6b26f75a2a88a",
     "grade": true,
     "grade_id": "cell-f560e22fdec3f7de",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful\n"
     ]
    }
   ],
   "source": [
    "# Test the KNN against known values. The test only covers small sample and there are hidden tests which will \n",
    "# tested after submitting. \n",
    "\n",
    "test_train = pd.read_csv('test.csv')\n",
    "test_test = pd.read_csv('test2.csv')\n",
    "\n",
    "train_labels = test_train['copium']\n",
    "train_features = test_train.drop(columns='copium')\n",
    "test_labels = test_test['copium']\n",
    "test_features = test_test.drop(columns='copium')\n",
    "\n",
    "knn = KNN(k=5)\n",
    "knn.fit(train_features, train_labels)\n",
    "pred_list = []\n",
    "\n",
    "for i in range(test_features.shape[0]):\n",
    "    pred = knn.predict(test_features.iloc[i])\n",
    "    pred_list.append(pred)\n",
    "val = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "if np.allclose(pred_list, val):\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9ba18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b998718e3379c65266541338c61c43f9",
     "grade": false,
     "grade_id": "cell-dde11302217e4fe7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## c) part 1: Evaluate the K-NN \n",
    "Evaluate the K-NN and choose a suitable k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f2a5c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b789d4ce7c7ba911a2bac909cdcef21f",
     "grade": true,
     "grade_id": "cell-6bd1e79f38dda753",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy 0.8683\n",
      "Precision 0.6216\n",
      "Recall 0.451\n"
     ]
    }
   ],
   "source": [
    "#train_labels = train['copium']\n",
    "#train_features = train.drop(columns='copium')\n",
    "\n",
    "train_labels = train_balanced_upsampled['copium']\n",
    "train_features = train_balanced_upsampled.drop(columns='copium')\n",
    "\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "\n",
    "# TODO, try differenent values of k. \n",
    "knn = KNN(k=1)\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "log = Classification_eval()\n",
    "for i in range(x.shape[0]):\n",
    "    pred = knn.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "\n",
    "print('Accuarcy', log.accuracy())\n",
    "print('Precision', log.precision())\n",
    "print('Recall', log.recall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545be3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e5e669568b89ae0d76cc4c8fa32f982",
     "grade": true,
     "grade_id": "cell-16eeb836b8b6e78b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Try some differnet values of k and just looking at these resutlts would the klassifier work well for all k? \n",
    "\n",
    "Answer:\\\n",
    "With the higher K values the accuracy is lower. This migh be because the higher K values may lead to generalisation issues in the model. However the precision value is higher and the recall value is lower which is better than the lower K values.\n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.8621 | 0.5897 | 0.415 |  \n",
    "| 5 | 0.8871 | 0.8261 | 0.3725 |   \n",
    "| 20 | 0.8527 | 1.0 | 0.0784 |   \n",
    "| 50 | 0.8495 | 1.0  | 0.0588 |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40aa1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71e823aa49829ee8019fa0faa47356f9",
     "grade": true,
     "grade_id": "cell-62d238ce7dd4e123",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## c) part 2, balanced data. \n",
    "Now try with the upsampled balanced data, test for the same k values as in part 1. Are the results different and whitch one would identify more copium?  \n",
    "\n",
    "Answer:\\\n",
    "There is a noticable improvement from the un-balanced data. However the precision and recall values are wors than the un-balanced data. This migh be fals posetive issue comping from the overrepresentation of posetive outcomes in the data set. \n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0.8683 | 0.6216 | 0.451 |  \n",
    "| 5 | 0.8307 | 0.4805 | 0.7255 |   \n",
    "| 20 | 0.8527  | 0.5323 | 0.6471 |   \n",
    "| 50 | 0.8464 | 0.5156 | 0.6471|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89904a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2ebdb355c2900e1ccbdddef700e9d07",
     "grade": false,
     "grade_id": "cell-9970ec54e795a716",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) kNN with Sklearn\n",
    "Typically one would not implement these algorithms by hand for each time you need them. Here we show how the kNN algorithm from sklearn can be used. Note, it does not normalize the features by default. The advantanges with the sklearn version is in computational cost as they utilize clever tree structures to minimize the search problem. \n",
    "\n",
    "A quick note: Even if ready packages exist, it is still very valuable to understand the principels behind the algorithms. For example when the learning doesn't work as well as you liked (happens more often then not...), you can reason to why and modify the problem or method into something that does work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2beeb5b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "974914833f9157cbd7879805a66d2af0",
     "grade": false,
     "grade_id": "cell-bafc1dac9bf2fe6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n",
      "Prediciton from our model 0  and from sklearn  [0]\n"
     ]
    }
   ],
   "source": [
    "# Set up data used for fitting the model\n",
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "# with the off the shelf model we need normalize the data before \n",
    "normalize = Normalize()\n",
    "data_features = normalize.update_normalization(train_features)\n",
    "\n",
    "# The test data\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "\n",
    "# Normalize the test data\n",
    "x_norm = normalize.normalize(x)\n",
    "\n",
    "# Set up an knn with sklearn \n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_sklearn.fit(data_features.values, train_labels.values)\n",
    "\n",
    "# Set up our own for comparison \n",
    "knn = KNN(k=5)\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# Test both for the 20 first values\n",
    "for i in range(20):\n",
    "    pred = knn.predict(x.iloc[i])\n",
    "    pred_sklearn = knn_sklearn.predict([x_norm.iloc[i]])\n",
    "    print('Prediciton from our model', pred, ' and from sklearn ',pred_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90f3e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d92f5d5dfaa4094e7cddec326d033f58",
     "grade": false,
     "grade_id": "cell-e0aa40fba01eeb67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 5: Tree based classifier\n",
    "\n",
    "In this exercise we will implement a thee based classifier. The implementation will be a class representing a node and will recursivly create child node until the entire tree structure is built. The general step for learning a tree structure from learning data is as follows:\n",
    "\n",
    "1. Initiate the the root node.\n",
    "2. Call the learn function with the learning data. \n",
    "\n",
    "Inside the learn function\n",
    "\n",
    "3. Check if all the data has the same classification, and check if the data set is smaller then min_node_size. If any of these are true, set the node as a leaf node and return. \n",
    "4. Find the spliting point in the data that returns the highest gini value, be careful to not use the label which we want to classify. \n",
    "5. Split the dataset, one is usually called head and the other tail. Save the splitting parameter and which value was used for the splitting. \n",
    "6. Check the prediction of the splited data sets (majority copium or not) and create two child nodes, one for each of the splited dataset. \n",
    "7. Call the the learn function of the child node with the corresponding split data and appdend the child node. After that return. \n",
    "\n",
    "The above will build the tree structure. To use the tree for prediction simply call the predict function of the root node with a data sample. In the predict function the following steps should happen.\n",
    "\n",
    "1. Check if the current node is a lead node, if it is simply return the self.prediction of that node. \n",
    "2. If it is not a leaf node then check the value of the splitting parameter in the data sample and compare with the splitting value. \n",
    "3. Call the predict function of the correspoinding child node with the data sample and return the answer.\n",
    "\n",
    "This will recurivly traverse the tree until a leaf node containing the prediction is reached. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e927e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b6dc4ca5e642577d97d027226ea8ca4",
     "grade": false,
     "grade_id": "cell-d05e3619e45ae5fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 a) Find split point\n",
    "\n",
    "First we define a function that can find the splitting point with the lowest gini value for a feature. For example, what is the best way to split the data with regards to the reflectivity feature. Page 34 in the book describes Gini index.  \n",
    "\n",
    "The gini value can be described as:\n",
    "\n",
    "If $\\Gamma$ contains the set of all labeles, then $\\Gamma(x < c)$ would be all labels that belong to the criteria $x < c$, where $x$ is a feature or parameter and $c$ is the splitting criteia (split_value).  \n",
    "\n",
    "\n",
    "For example: \n",
    "\n",
    "If $x$ is ironRate and our splitting critera is $c = 0.4$, then $\\Gamma(x < c)$ would be a set of all the label from samples with $\\textrm{ironRate} < 0.4$. Remeber that our labels are either 0 or 1 in this assignment. \n",
    "\n",
    "\n",
    "Then we can define:\n",
    "\n",
    "$r_1 = mean(\\Gamma(x < c))$\n",
    "\n",
    "$r_2 = mean(\\Gamma(x \\geq c))$\n",
    "\n",
    "Gini index left side\n",
    "\n",
    "$q_1 = 2r_1(1-r_1)$\n",
    "\n",
    "Gini index right side\n",
    "\n",
    "$q_2 = 2r_2(1-r_2)$\n",
    "\n",
    "We define $len(\\Gamma)$ to give the number of samples in the dataset, then the weighted gini value is:\n",
    "\n",
    "$q = \\frac{len(\\Gamma(x_i < c))}{len(\\Gamma)}*q_1 + \\frac{len(\\Gamma(x_i \\geq c))}{len(\\Gamma)}*q_2$\n",
    "\n",
    "The goal is to find a criteria $c$ that minimizes the gini value $q$ with regards to a feature. One way to do it is to sort the data with regards to the feature, this makes $\\Gamma(x < c)$ easier. We could then try every split point with a for loop and save the one that has the lowes gini value. The data is always split beteen two data points therefore the critieria c would be the average between the two data points that are sorted, i.e. $c_i = \\frac{x_{i-1} + x_i}{2}$. First split has index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa24f570",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcbb3d07e1d26d0fc781938cb5c228d5",
     "grade": false,
     "grade_id": "cell-caa92101f7dd232d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_split_point(data, label, feature):\n",
    "    \"\"\"\n",
    "    data - all the data we want to split, our (gamma)\n",
    "    label - the parameter we want to classify. \n",
    "    feature - the feature we want to check for, our x_i, could for example be iron\n",
    "    -----------\n",
    "    retrun:\n",
    "    split_value - the spliting value, our c. \n",
    "    gini_value - the gini value for the best c.\n",
    "    df_head - the data frame belonging to x_i < c\n",
    "    df_tail - the data frame belonging to x_i => c\n",
    "    \"\"\"\n",
    "    # Beging by sorting the data after the paramter. \n",
    "    sorted_data = data.sort_values(by=feature)\n",
    "    sorted_label = sorted_data[label]\n",
    "    \n",
    "    # TODO loop through all the split points in the sorted data and find \n",
    "    # the best gini_value (s) and split_value (c).\n",
    "    # the functions .head(split_index) and .tail(len(x) - split_index) could be useful.\n",
    "    c_list = [(sorted_data[feature].iloc[i-1]+sorted_data[feature].iloc[i])/2 for i in range(1,sorted_data.shape[0])]\n",
    "    q_vals = []\n",
    "    for c_index in range(len(c_list)):\n",
    "        split_index = c_index + 1\n",
    "        r1 = np.mean(sorted_label.head(split_index))\n",
    "        #print(r1)\n",
    "        r2 = np.mean(sorted_label.tail(len(c_list) - split_index))\n",
    "\n",
    "        q1 = 2*r1*(1-r1)\n",
    "        q2 = 2*r2*(1-r2)\n",
    "\n",
    "        q_vals.append(((sorted_label.head(split_index).shape[0])/(sorted_label.shape[0]))*q1 \n",
    "             + ((sorted_label.tail(len(c_list) - split_index).shape[0])/(sorted_label.shape[0]))*q2)\n",
    "\n",
    "    sorted_q = np.argsort(q_vals)\n",
    "    c_index = sorted_q[0]\n",
    "    split_value = c_list[c_index]\n",
    "    gini_value = q_vals[c_index]\n",
    "    split_value = c_list[c_index]\n",
    "    split_index = c_index+1\n",
    "\n",
    "    for i in q_vals:\n",
    "        if i ==0.27655122655122655:\n",
    "            print(\"AWIHAUWDHAOIPAWDHKLAHDN√ñOAWHIDN\")\n",
    "   \n",
    "    # TODO: get the two data frames the corresponds to the split data. \n",
    "    # the functions .head(split_index) and .tail(len(x) - split_index) could be useful. \n",
    "    df_head = sorted_data.head(split_index)\n",
    "    df_tail = sorted_data.tail(len(c_list) - split_index)\n",
    " \n",
    "    \n",
    "    return split_value, gini_value, df_head, df_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accf1ba9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c766b12e2c258f295fc271d5a360a366",
     "grade": true,
     "grade_id": "cell-cb0e81d04a850fb0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed\n",
      "split_value =  1.8762144694722296\n",
      "gini_value =  0.2714491857349\n"
     ]
    }
   ],
   "source": [
    "# Test for find_split_point function, the test does not test everything and more hidden tests are used \n",
    "# during grading. \n",
    "df_test = pd.read_csv('test.csv')\n",
    "split_value, gini_value, df_head, df_tail = find_split_point(df_test, 'copium', 'ground_density')\n",
    "\n",
    "if split_value == 1.8762144694722296 and gini_value == 0.27655122655122655:\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    print(\"split_value = \", split_value)\n",
    "    print(\"gini_value = \", gini_value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f30de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2223aa51cfa949a14d97613d8480f74",
     "grade": false,
     "grade_id": "cell-d311f53e9383d913",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 b) Tree Node\n",
    "\n",
    "Here we will code a tree node class that can recursivly call it self. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d660a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9681ba86087680dccbe1525ca1cafcf4",
     "grade": false,
     "grade_id": "cell-5d724b8a3a41de81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 9 (2435152027.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    def learn(self, data, label, min_node_size):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 9\n"
     ]
    }
   ],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, prediction=None):\n",
    "        self.split_value = None # the splitting value (c)\n",
    "        self.split_feature = None # what feature where uesd for the split (x_i)\n",
    "        self.child_nodes = [] # list that contains two child nodes, if not leaf_node\n",
    "        self.leaf_node = 0 # is this leaf_node (0=no, 1=yes)\n",
    "        self.prediction = prediction # classification made in this node.\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: To make a prediction we need to traverse the tree recursivly down to a leaf node and return \n",
    "        # the prediction. \n",
    "        # step 1: check if this is a leaf node, if it is then return prediction of this node, \n",
    "        # otherwise contine with step 2.\n",
    "        # step 2: Compare the input data with the splitting criteria, i.e. the is the value of the data smaller \n",
    "        # or larger then the split value.\n",
    "        # step 3: Call the prediction function in the corresponding child_node and return the prediction. \n",
    "        \n",
    "            \n",
    "    def learn(self, data, label, min_node_size):\n",
    "        \"\"\"\n",
    "        data - the training data\n",
    "        label - the parameter we want to classify, i.e. \"copium\"\n",
    "        min_node_size - number of data points in a node for it to become a leaf node. \n",
    "        \"\"\"\n",
    "        # TODO: wirte the learning function. \n",
    "\n",
    "        # Step 1: check if this node should be turned into a leaf node, this happens if the data set is smaller \n",
    "        # then the min_node_size or if the data labels are homogenious, i.e. avg = 0 or 1. \n",
    "        \n",
    "        # Step 2: Loop over all features and get the best gini and split_value. Make sure you are not \n",
    "        # using the label as a feature. \"data.columns\" can give you all features plus the label \n",
    "        # Save the best split value and split_paramter and the two new data frames \n",
    "        # corresponding to the split [df_head, df_tail] (these will be data frames for the child nodes).  \n",
    "        \n",
    "        \n",
    "        # Step 3: Calculate the majority vote prediction of the two data frames. \n",
    "        # Create two child nodes, (e.g. child = TreeNode(prediction=1) for prediction copium) and and call the \n",
    "        # learn function with the corresponding dataframe. \n",
    "        \n",
    "        # Step 4: append the the child node to the self.child_nodes. It should be in the order \n",
    "        # of the child node correspoinding to [df_head, df_tail].\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93629c10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d029c5362d95372b9dc7438482a6021f",
     "grade": true,
     "grade_id": "cell-cfb150cc7e9ef0dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test to give indicaion if the TreeNode class is correct.\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "tree = TreeNode() # create root node\n",
    "# learn the tree structure\n",
    "tree.learn(df_test, \"copium\", min_node_size=4)\n",
    "\n",
    "pred_list = []\n",
    "pred_answer = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, \n",
    "               1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, \n",
    "               0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    pred = tree.predict(df_test.iloc[i])\n",
    "    pred_list.append(pred)\n",
    "    \n",
    "if np.allclose(pred_list, pred_answer):\n",
    "    print('Test successful')\n",
    "else:\n",
    "    print('Test failed')\n",
    "    print('Prediction', pred_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbd608",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "329602f64e3ee9fb6de3d4c97197c96b",
     "grade": false,
     "grade_id": "cell-7949933930575680",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Train the Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23eef95",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42725500fbda2583a59bf5e7d91cef73",
     "grade": true,
     "grade_id": "cell-d41091cb1d65774f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tree = TreeNode() # create root node\n",
    "# learn the tree structure\n",
    "tree.learn(train, \"copium\", min_node_size=2)\n",
    "#tree.learn(train_balanced_upsampled, \"copium\", min_node_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abee52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "484c482cce5a8adb2cdc9892b028d18d",
     "grade": false,
     "grade_id": "cell-959bd6f694aaadc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "log = Classification_eval()\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    pred = tree.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "        \n",
    "print('accuarcy', log.accuracy())\n",
    "print('precision', log.precision())\n",
    "print('recall', log.recall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33168109",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "340eacb4b6651c76177c99abf4fe91a6",
     "grade": true,
     "grade_id": "cell-5ac1365691381dae",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## 5 c) part 1\n",
    "\n",
    "Use the non balanced data to learn the tree classifier and test the classifier for some different min_node_size values.\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 2  |  |  |  |  \n",
    "| 10 |  |  |  |   \n",
    "| 20 |  |  |  |   \n",
    "| 50 |  |  |  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2713c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5210421da249c73783c1bdfd4d505f4a",
     "grade": true,
     "grade_id": "cell-6a1e781f28a063e1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## 5 c) part 2, Try with balanced data\n",
    "\n",
    "Use the balanced data to learn the tree classifier and test the classifier for some different min_node_size values.\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 2  |  |  |  |  \n",
    "| 10 |  |  |  |   \n",
    "| 20 |  |  |  |   \n",
    "| 50 |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2830e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edb35a095a76dc2633f2d51b4bbc3e7c",
     "grade": false,
     "grade_id": "cell-4f6685679c5b4be9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) Tree classifier with sklearn \n",
    "\n",
    "Here we compare our tree classifier to the one implemented in sklearn. It is important to note that there can be differences between them, there can be minor implementation details that can affect the result, for example, there is some randomness in the sklearn implementaion. But overall they give simlar results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6916f57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82dcbad808d4dc13f37c755e3d6c432e",
     "grade": false,
     "grade_id": "cell-6e90f7b38bbbe9f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up data used for fitting the model\n",
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "# Decition tree or classifier from sklearn \n",
    "tree_sklearn = DecisionTreeClassifier(min_samples_split=2, splitter='best', \n",
    "                             criterion='gini', min_samples_leaf=1)\n",
    "# Train the sklearn tree classifer\n",
    "tree_sklearn.fit(train_features.values, train_labels.values)\n",
    "\n",
    "# Train our tree classifier \n",
    "tree = TreeNode() # create root node\n",
    "tree.learn(train, \"copium\", min_node_size=2)\n",
    "\n",
    "# Test both and compare\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "# classify the data\n",
    "for i in range(30):\n",
    "    pred_sklearn = tree_sklearn.predict([x.iloc[i]])\n",
    "    pred = tree.predict(x.iloc[i])\n",
    "\n",
    "    print('sklearn pred ',pred_sklearn, 'our tree pred ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19598c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23d04d944edbe4b67aa9058bbb98b5ad",
     "grade": false,
     "grade_id": "cell-f59f64a8c41e294f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 6: Balance data analysis\n",
    "\n",
    "1. It there a difference between that results of the balanced and non-balanced data for the tree classifier?\n",
    "2. Is there a difference between the K-NN vs tree classifier?\n",
    "3. Which one would you expect to be a faster classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231f853",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23d8998f99e14c18babaa22afc7f2aa3",
     "grade": true,
     "grade_id": "cell-f67f58809daadcb6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3840d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edce31490be1574477c5465dded60200",
     "grade": false,
     "grade_id": "cell-fa3691f04f48c770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exersice 7: Deployment (Optional)\n",
    "\n",
    "Here we will try the learned classifiers on a larger map. Make sure that the last run version of K-NN and tree have good parameters i.e. k and min_node_size values. Which one found more copium?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b45b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aeb0fc0ae619e677ddedb5046c52706",
     "grade": true,
     "grade_id": "cell-c375db8a210a8a7d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = Environment(map_type=2, fps=5, resolution=(1000, 1000))\n",
    "\n",
    "try:\n",
    "    sensor_properties = env.get_sensor_properties()\n",
    "    sensor_sample = dict()\n",
    "    for key in sensor_properties:\n",
    "        sensor_sample[key] = [0]\n",
    "\n",
    "    log_knn = Classification_eval()\n",
    "    log_tree = Classification_eval()\n",
    "\n",
    "\n",
    "    for i in range(500):\n",
    "        action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "        env.step(action)\n",
    "        if env.get_sensor_readings() is not None:\n",
    "            sensor_readings = env.get_sensor_readings()\n",
    "            for key in sensor_readings:\n",
    "                sensor_sample[key][0] = sensor_readings[key]\n",
    "            sensor_sample_df = pd.DataFrame(sensor_sample)\n",
    "            log_knn.update(knn.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "            log_tree.update(tree.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "            env.plt_acc.update_acc(log_tree.accuracy(), log_knn.accuracy())\n",
    "        env.render()\n",
    "\n",
    "    env.exit()\n",
    "\n",
    "    print(\"K-NN accuracy \", log_knn.accuracy(), \"Tree accuracy\", log_tree.accuracy())\n",
    "    print(\"Number of copium deposits found, K-NN:\", log_knn.TP, \" Tree:\", log_tree.TP)\n",
    "except:\n",
    "    env.exit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf6c50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
